{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx0D-Htol5sN"
   },
   "source": [
    "# Single-taks PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z1tJsW5oljIA",
    "outputId": "9768071e-1826-429a-e434-8d11565ffda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git\n",
      "  Cloning https://github.com/corl-team/xland-minigrid.git to /tmp/pip-install-j09355fw/xminigrid_49a68358bba44f51bedfedf85d836ea7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/corl-team/xland-minigrid.git /tmp/pip-install-j09355fw/xminigrid_49a68358bba44f51bedfedf85d836ea7\n",
      "  Resolved https://github.com/corl-team/xland-minigrid.git to commit 991f13c7885c24c82302a1ee3a68a24a29801a94\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: jax>=0.4.16 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.4.26)\n",
      "Requirement already satisfied: jaxlib>=0.4.16 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.4.26+cuda12.cudnn89)\n",
      "Requirement already satisfied: flax>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.8.3)\n",
      "Requirement already satisfied: rich>=13.4.2 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (13.7.1)\n",
      "Requirement already satisfied: chex>=0.1.85 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.1.86)\n",
      "Requirement already satisfied: imageio>=2.31.2 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.31.6)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.4.9)\n",
      "Collecting matplotlib>=3.7.2 (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb>=0.15.10 (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyrallis>=0.3.1 (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading pyrallis-0.3.1-py3-none-any.whl (33 kB)\n",
      "Collecting distrax>=0.1.4 (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: optax>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.2.2)\n",
      "Collecting orbax>=0.1.9 (from xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading orbax-0.1.9.tar.gz (1.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.85->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.85->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.85->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.25.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.85->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.12.1)\n",
      "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from distrax>=0.1.4->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.23.0)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.0.8)\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.4.4)\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.1.45)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (6.0.1)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.31.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (9.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.4.9->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (67.7.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.8.2)\n",
      "Collecting typing-inspect (from pyrallis>=0.3.1->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.4.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.4.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.16.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (5.9.5)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.31.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle (from wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.4.2->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.1.2)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.7.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (1.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2024.2.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.4->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (4.4.2)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.4->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.4->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.5.4)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.4->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (0.1.8)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyrallis>=0.3.1->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.10->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (2023.6.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (6.4.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid[baselines]@ git+https://github.com/corl-team/xland-minigrid.git) (3.18.1)\n",
      "Building wheels for collected packages: orbax, xminigrid\n",
      "  Building wheel for orbax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for orbax: filename=orbax-0.1.9-py3-none-any.whl size=1498 sha256=ba52a0df4a16ec96f00b45cd040e15d1b2b401731dd389cfa7397da65bcb7284\n",
      "  Stored in directory: /root/.cache/pip/wheels/14/7a/98/b955a4db98b54317c311ee32367994ca530721c62a87ec56a7\n",
      "  Building wheel for xminigrid (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for xminigrid: filename=xminigrid-0.8.0-py3-none-any.whl size=56130 sha256=9c72874d1827e5b5fcb3298f9a6f418302eb4a5407234c92ac4248c34f4d88c3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dgz1sp37/wheels/eb/f2/ca/6a50e90c56da3278ffb675a57b98ef22e67233965e39fe279c\n",
      "Successfully built orbax xminigrid\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, mypy-extensions, docker-pycreds, typing-inspect, matplotlib, gitdb, pyrallis, gitpython, wandb, orbax, distrax, xminigrid\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "Successfully installed distrax-0.1.5 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 matplotlib-3.8.4 mypy-extensions-1.0.0 orbax-0.1.9 pyrallis-0.3.1 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 typing-inspect-0.9.0 wandb-0.17.0 xminigrid-0.8.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "43b3d06aa69a47e781c7fcd73679a6ff",
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jax is already installed on the colab, uncomment only if needed\n",
    "# !pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n",
    "# !pip install 'xminigrid[baselines]'\n",
    "!pip install \"xminigrid[baselines] @ git+https://github.com/corl-team/xland-minigrid.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_ZnsIzPl--m"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from typing import TypedDict, Optional\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import distrax\n",
    "import optax\n",
    "import imageio\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flax import struct\n",
    "from flax.linen.initializers import glorot_normal, orthogonal, zeros_init\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from dataclasses import asdict, dataclass\n",
    "from functools import partial\n",
    "\n",
    "import xminigrid\n",
    "from xminigrid.environment import Environment, EnvParams\n",
    "from xminigrid.wrappers import GymAutoResetWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuWQxjz9rHDP"
   },
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykIEmHzXqq2D"
   },
   "outputs": [],
   "source": [
    "# Model adapted from minigrid baselines:\n",
    "# https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "\n",
    "# custom RNN cell, which is more convenient than default in flax\n",
    "# GRU helps solve the vanishing gradient problem and can capture dependencies from long input seq\n",
    "class GRU(nn.Module):  # inherits from `nn.Module` which is a base class for all NN in Flax\n",
    "    hidden_dim: int  # attribute of the GRU class, size of the hidden state of the GRU cell\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, xs, init_state):  # allows the GRU class instances to be called like a function\n",
    "        seq_len, input_dim = xs.shape  # extracts the sequence length and input dimension from the shape of the input tensor `xs`\n",
    "\n",
    "        # this init might not be optimal, for example, bias for reset gate should be -1 (for now ok)\n",
    "\n",
    "        # weight matrix for input, used to transform the input `x` at each step\n",
    "        # the shape `(hidden_dim * 3, input_dim)` reflects the three gates in a GRU cell: reset, update, and new gates\n",
    "        # `glorot_normal`, also known as Xavier normal initializer, used here for `Wi` to help keeping the gradient\n",
    "        # magnitudes in a reasonable range during the beginning of training\n",
    "        Wi = self.param(\"Wi\", glorot_normal(in_axis=1, out_axis=0), (self.hidden_dim * 3, input_dim))\n",
    "\n",
    "        # weight matrix for the hidden state, used to transform the previous hidden state `h`\n",
    "        # same reasoning for the size `(hidden_dim * 3, hidden_dim)`\n",
    "        # `orthogonal` used for `Wh`, ensuring that the different hidden units start off in a decorrelated manner,\n",
    "        # which can help in learning diverse features in the early stages\n",
    "        Wh = self.param(\"Wh\", orthogonal(column_axis=0), (self.hidden_dim * 3, self.hidden_dim))\n",
    "\n",
    "        # bias vector for the input transformations\n",
    "        # the shape `(hidden_dim * 3,)` supports the three gates\n",
    "        # `zeros_init()` initializes biases with zeros\n",
    "        bi = self.param(\"bi\", zeros_init(), (self.hidden_dim * 3,))\n",
    "\n",
    "        # bias for the new gate transformation, specially after applying the rest gate operation\n",
    "        # `zeros_init()` initializes biases with zeros\n",
    "        bn = self.param(\"bn\", zeros_init(), (self.hidden_dim,))\n",
    "\n",
    "        # this is where the actual computations for the GRU gates occur at each step in the input seq\n",
    "        def _step_fn(h, x):  # h is the current hidden state of the GRU cell, x is the current input at this step of the seq\n",
    "\n",
    "            # gate pre-activations\n",
    "            igates = jnp.split(Wi @ x + bi, 3)  # input transformations, three parts of the result corresponding to one of GRU's gates before applying activations functions\n",
    "            hgates = jnp.split(Wh @ h, 3)  # hidden state transformations\n",
    "\n",
    "            # gate activations\n",
    "\n",
    "            # controls how much of the past info (prev hidden state) will be forgotten\n",
    "            reset = nn.sigmoid(igates[0] + hgates[0])  #  output between 0 (forget the past) and 1 (keep past info)\n",
    "            # determines how much of old hidden state should be retained and how much of the new candidate state should be used to update the hidden state\n",
    "            update = nn.sigmoid(igates[1] + hgates[1])  # output between 0 and 1\n",
    "            # candidate activation for the hidden state at the current timestep; the rest modulation allows the cell to drop info from the prev state if the reset gate is activated\n",
    "            new = nn.tanh(igates[2] + reset * (hgates[2] + bn))  # output between -1 and 1\n",
    "            # hidden state update; if update close to 1, the new state is similar to the old state\n",
    "            next_h = (1 - update) * new + update * h\n",
    "\n",
    "            return next_h, next_h  # this is for jax.lax.scan, as the function must return a tuple of two elem (an updated carry and an output elem)\n",
    "\n",
    "        # lax.scan iterates over the seq `xs`, and for each `x` in `xs`, it calls `_step_fn(h, x)`, where `h` is the hidden state carried from the prev iteration\n",
    "        # then `_step_fn` calculates the next hidden state based on the current hidden state and input; this updated state is then passed as the hidden state for the next iter\n",
    "        # `all_states`: for each application of `_step_fn`, alongside updating the carry (hidden state), it also produces and output (in this case, the same updated hidden state)\n",
    "        # `lax.scan` collects these outputs across all iterations into the array `all_states`. This array will contain the seq of all hidden states computed for each timestep\n",
    "        # `last_state`: after processing all elem of `xs`, the final output of the carry (last output from `_step_fn`) is returned as `last_state`\n",
    "        last_state, all_states = jax.lax.scan(_step_fn, init=init_state, xs=xs)\n",
    "        return all_states, last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SlfEcuGXzdY"
   },
   "outputs": [],
   "source": [
    "# construct a multi-later RNN using our custum GRU cells\n",
    "class RNNModel(nn.Module):\n",
    "  hidden_dim: int  # dim of the hidden state for each GRU cell in the network\n",
    "  num_layers: int  # num of GRU layers in the model\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, xs, init_state):\n",
    "      # xs: [seq_len, input_dim]\n",
    "      # init_state: [num_layers, hidden_dim]\n",
    "      outs, states = [], []  # `outs` collect the outputs from each layer; `states` collect the final states from each layer\n",
    "\n",
    "      # iterates through each GRU layer\n",
    "      for layer in range(self.num_layers):\n",
    "          xs, state = GRU(hidden_dim=self.hidden_dim)(xs, init_state[layer])\n",
    "          outs.append(xs)\n",
    "          states.append(state)\n",
    "\n",
    "      # sum outputs from all layers, kinda like in ResNet\n",
    "      # the summed outputs is from all layers, providing a single output seq where each elem is an aggregate of info from all layers\n",
    "      # layer states is the array of final states from each layer, which is useful for continuing the seq processing in subsequent steps or for debugging\n",
    "      return jnp.array(outs).sum(0), jnp.array(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idefUkJSbSpt"
   },
   "outputs": [],
   "source": [
    "# transform `RNNModel` into a batch-processing model, enabling parallel processing of multiple sequences\n",
    "# `variable_axes` specifies how the axes of the param should be treated when vectorizing; setting `{\"params\": None}` indicates that the param do not get an added batch dim (the same set of param is used across all items in the batch)\n",
    "# `split_rngs`, setting `{\"params\": False}` means that the RNGs used in the model (if any) are not split along the batch axis, meaning that the same RNG state is used across the entire batch\n",
    "BatchedRNNModel = flax.linen.vmap(\n",
    "    RNNModel, variable_axes={\"params\": None}, split_rngs={\"params\": False}, axis_name=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ1Rr4ua9fFA"
   },
   "outputs": [],
   "source": [
    "class ActorCriticInput(TypedDict):\n",
    "    observation: jax.Array\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array\n",
    "\n",
    "class ActorCriticRNN(nn.Module):\n",
    "    num_actions: int\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 64\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 64\n",
    "    img_obs: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: ActorCriticInput, hidden: jax.Array) -> tuple[distrax.Categorical, jax.Array, jax.Array]:\n",
    "        B, S = inputs[\"observation\"].shape[:2]\n",
    "        # encoder from https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "        if self.img_obs:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    nn.Conv(16, (3, 3), strides=2, padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(32, (3, 3), strides=2, padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(32, (3, 3), strides=2, padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(32, (3, 3), strides=2, padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    nn.Conv(16, (2, 2), padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(32, (2, 2), padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(64, (2, 2), padding=\"VALID\", kernel_init=orthogonal(math.sqrt(2))),\n",
    "                    nn.relu,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # the action encoder transforms discrete action indices into a continuous embedding space\n",
    "        # `self.num_actions` is the total number of possible actions, which defines the size of the input dimension to the embedding layer\n",
    "        # `self.action_emb_dim` is the dimensionality of the embedding space, which is the size of the vector that each action will be transformed into\n",
    "        action_encoder = nn.Embed(self.num_actions, self.action_emb_dim)\n",
    "\n",
    "        # RNN core processes sequences of embedded observations, actions, and possibly other inputs through multiple layers of an RNN\n",
    "        rnn_core = BatchedRNNModel(self.rnn_hidden_dim, self.rnn_num_layers)\n",
    "\n",
    "        # actor network computes a set of action logits based on the RNN's output\n",
    "        # these logits will determine the policy by defining the probability distribution over possible actions\n",
    "        actor = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(self.head_hidden_dim, kernel_init=orthogonal(2)),  # orthogonal initializer helps in maintaining the diversity of initial weights and prevents the vanishing/exploding gradient problem\n",
    "                nn.tanh,\n",
    "                nn.Dense(self.num_actions, kernel_init=orthogonal(0.01)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # critic network estimates the value function from the state represented by the RNN's output\n",
    "        # this value estimation is crucial for calculating the advantages used in training the policy\n",
    "        # theh final layer outputs a single value, representing the expected return (value function) from the current state\n",
    "        critic = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(self.head_hidden_dim, kernel_init=orthogonal(2)),\n",
    "                nn.tanh,\n",
    "                nn.Dense(1, kernel_init=orthogonal(1.0)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # [batch_size, seq_len, ...]\n",
    "        # `img_encoder` processes the raw observation; after encoding, the data is reshaped\n",
    "        # `-1` allows the array to automatically calculate the necessary size to maintain the same total number of elements within each sample in the batch\n",
    "        obs_emb = img_encoder(inputs[\"observation\"]).reshape(B, S, -1)\n",
    "        # this transforms the previous actions into a dense vector representation using an embedding layer\n",
    "        act_emb = action_encoder(inputs[\"prev_action\"])\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim + act_emb_dim + 1]\n",
    "        # concatenates the embedded observations, actions, and rewards into a single tensor `out`\n",
    "        # `inputs[\"prev_reward\"][..., None]` extends the reward tensor along a new axis so it can be concatenated with the observation and action embeddings\n",
    "        # overall, this concatenates along the last axis, effectively creating a single feature vector per timestep that combines info from observations, actions, and rewards\n",
    "        out = jnp.concatenate([obs_emb, act_emb, inputs[\"prev_reward\"][..., None]], axis=-1)\n",
    "\n",
    "        # core networks\n",
    "        # processing with RNN core: the RNN takes the current input and the previous hidden state `hidden` to produce the output for the current timestep and the new hidden state for the next timestep\n",
    "        out, new_hidden = rnn_core(out, hidden)\n",
    "        # actor and critic networks\n",
    "        # passes the RNN output through the actor network, which predicts a set of action logits.\n",
    "        # These logits are used to form a probability distribution over possible actions\n",
    "        # then we convert the logits into a categorical distribution, which is used for sampling actions during training and inference\n",
    "        dist = distrax.Categorical(logits=actor(out))\n",
    "        # the same RNN output is also passed to the critic network, which outputs a value estimate\n",
    "        # this estimate represents the expected sum of future rewards from the current state, crucial for the training of the actor network via policy gradients\n",
    "        values = critic(out)\n",
    "\n",
    "        return dist, jnp.squeeze(values, axis=-1), new_hidden\n",
    "\n",
    "    # `carry` in JAX is just initial hidden state\n",
    "    def initialize_carry(self, batch_size):\n",
    "        return jnp.zeros((batch_size, self.rnn_num_layers, self.rnn_hidden_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXf61mXqwPek"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clrxOo4XwQho"
   },
   "outputs": [],
   "source": [
    "class Transition(struct.PyTreeNode):\n",
    "    done: jax.Array\n",
    "    action: jax.Array\n",
    "    value: jax.Array\n",
    "    reward: jax.Array\n",
    "    log_prob: jax.Array\n",
    "    obs: jax.Array\n",
    "    # for rnn policy\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imRmFjCu0f96"
   },
   "outputs": [],
   "source": [
    "# calculate the Generalized Advantage Estimation\n",
    "# the advantage function helps indicate how much better taking a particular action is compared to the policy's average\n",
    "def calculate_gae(\n",
    "    transitions: Transition,  # encapsulate the state transitions including rewards, values, actions\n",
    "    last_val: jax.Array,  # the value estimate of the terminal state of the sequence, used to bootstrap the advantage estimation for the last transition\n",
    "    gamma: float,  # discount factor to discount future rewards\n",
    "    gae_lambda: float,  # smoothing param for GAE, controls the bias-variance tradeoff in the estimation; a lambda closer to 1 results in lower bias and higher variance\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "    # single iteration for the loop\n",
    "    # this inner function is designed to compute the advantage at each timestep, working backward from the last timestep to the first\n",
    "    def _get_advantages(gae_and_next_value, transition):  # `gae_and_next_value` is a tuple containing the current estimate of GAE and the value of the next state; `transition` is the transition data for the current timestep\n",
    "        gae, next_value = gae_and_next_value\n",
    "        # `delta` is the temporal difference error for the current state; (1 - transition.done) indicates whether the state is terminal\n",
    "        delta = transition.reward + gamma * next_value * (1 - transition.done) - transition.value\n",
    "        gae = delta + gamma * gae_lambda * (1 - transition.done) * gae\n",
    "        return (gae, transition.value), gae\n",
    "\n",
    "    # apply `_get_advantages` across all transitions and processing them in reverse\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        transitions,\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    # advantages and values (Q)\n",
    "    # advantages are used to weight the policy gradient updates, helping to focus learning on the most beneficial actions\n",
    "    # values (Q) is used for actor-critic updates\n",
    "    return advantages, advantages + transitions.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Emi1NDVNmUrV"
   },
   "outputs": [],
   "source": [
    "# to perform a single update of the network (both actor and critic) in a PPO setup using a batch of transitions collected from the environment\n",
    "def ppo_update_networks(\n",
    "    train_state: TrainState,\n",
    "    transitions: Transition,\n",
    "    init_hstate: jax.Array,  # initial hidden states of the RNN\n",
    "    advantages: jax.Array,   # advantages for eact action, used to weight the updates to the policy\n",
    "    targets: jax.Array,      # target values for the value function, used to compute the value loss\n",
    "    clip_eps: float,         # the clipping parameter for PPO, helps control the change to the policy by clipping the ratio of the new to old policy probabilities\n",
    "    vf_coef: float,          # coefficient for the value loss in the total loss calculation, balancing the importance of the value loss relative to other components\n",
    "    ent_coef: float,         # coefficient for the entropy bonus, encouraging exploration by rewarding higher entropy in the policy distribution\n",
    "):\n",
    "    # NORMALIZE ADVANTAGES\n",
    "    # this reduces variance in training and scales the updates during training\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # computes the losses for both the actor and critic networks\n",
    "    def _loss_fn(params):\n",
    "        # RETURN NETWORK\n",
    "        # apply the network model to get the distribution, value predictions, and potentially updated hidden states from the current params and observations\n",
    "        dist, value, _ = train_state.apply_fn(\n",
    "            params,\n",
    "            {\n",
    "                # [batch_size, seq_len, ...]\n",
    "                \"observation\": transitions.obs,\n",
    "                \"prev_action\": transitions.prev_action,\n",
    "                \"prev_reward\": transitions.prev_reward,\n",
    "            },\n",
    "            init_hstate,\n",
    "        )\n",
    "        # log probability of the taken actions under the current policy\n",
    "        log_prob = dist.log_prob(transitions.action)\n",
    "\n",
    "        # CALCULATE VALUE LOSS\n",
    "        # for updating the critic component of the actor-critic architecture\n",
    "        # the critic's job is to estimate the value function, which quantifies the expected return from a given state under the current policy\n",
    "        # `transitions.value` is the value estimates at each state as computed by the critic network at the time the actions were taken (old value estimates)\n",
    "        # `value` is the current value estimates computed using the updated critic network parameters based on the current model's weights\n",
    "        # the difference between the new value estimates and the old ones (`value - transitions.value`) is clipped to lie within the range `[-clip_eps, clip_eps]`\n",
    "        # the clipping is done to control the magnitude of the update to the value function, preventing large jumps that can destabilize training by ensuring that the new value predictions do not deviate too much from the old predictions\n",
    "        value_pred_clipped = transitions.value + (value - transitions.value).clip(-clip_eps, clip_eps)\n",
    "        value_loss = jnp.square(value - targets)\n",
    "        value_loss_clipped = jnp.square(value_pred_clipped - targets)\n",
    "        # for each instance, the higher of the two losses (unlipped and clipped value losses) is used\n",
    "        # this ensures that the loss reflects the worst-case scenario between moving too far from the old value estimate and diverging from the target value\n",
    "        # `0.5 *` scales down the loss, which is common in quadratic loss functions to balance the gradient magnitudes during backpropogation\n",
    "        value_loss = 0.5 * jnp.maximum(value_loss, value_loss_clipped).mean()\n",
    "\n",
    "        # CALCULATE ACTOR LOSS\n",
    "        # key to updating the policy in a way that improves decision-making while controlling for overly aggressive updates\n",
    "        # `log_prob` is the log probability of the actions as predicted by the current policy\n",
    "        # `transitions.log_prob` is the log probability of the actions as predicted by the policy at the time the action was taken (stored from when the transitions were generated)\n",
    "        ratio = jnp.exp(log_prob - transitions.log_prob)\n",
    "        # this part of the actor loss is directly proportional to the advantage and the probability ratio; represents how much better or worse the action is under the new policy compared to the old, weighted by the advantage of each action\n",
    "        actor_loss1 = advantages * ratio\n",
    "        # applies clipping to the ratio to ensure that the updates are bounded\n",
    "        actor_loss2 = advantages * jnp.clip(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "        # final actor loss; minimum here ensures that the loss used does not exceed the clipped version\n",
    "        actor_loss = -jnp.minimum(actor_loss1, actor_loss2).mean()\n",
    "        # `entropy` is calculated from the distribution which reflects the policy's uncertainty. A higher entropy suggests more randomness in the action selection, which can be beneficial for exploration\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        # `vf_coef` (value function coefficient) determines how much weight the value loss has relative to the actor loss in the total loss calculation\n",
    "        # the value loss helps in accurately estimating how good each state-action pair is\n",
    "        # `- ent_coef * entropy` acts as a regularization term that encourages exploration by rewarding higher policy entropy\n",
    "        total_loss = actor_loss + vf_coef * value_loss - ent_coef * entropy\n",
    "\n",
    "        return total_loss, (value_loss, actor_loss, entropy)\n",
    "\n",
    "    # `jax.value_and_grad` computes both the value of a function and its gradient wrt its inputs\n",
    "    # `grads` here is the gradients of the total loss wrt the model params, which are used to update the model in the training step\n",
    "    (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(train_state.params)\n",
    "    # `jax.lax.pmean` performs a \"parallel mean\" across all devices specified by the `axis_name=\"device\"`; this ensures that each device gets the average of the computed values and gradients across all devices\n",
    "    (loss, vloss, aloss, entropy, grads) = jax.lax.pmean((loss, vloss, aloss, entropy, grads), axis_name=\"devices\")\n",
    "    # it modifies the model params in a way that ideally reduces the total loss on the next eval of the model\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # PACKING UPDATE INFO\n",
    "    update_info = {\n",
    "        \"total_loss\": loss,\n",
    "        \"value_loss\": vloss,\n",
    "        \"actor_loss\": aloss,\n",
    "        \"entropy\": entropy,\n",
    "    }\n",
    "\n",
    "    return train_state, update_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du9H4tYxQ6w0"
   },
   "outputs": [],
   "source": [
    "# for evaluation (evaluate for N consecutive episodes, sum rewards)\n",
    "# N=1 single task, N>1 for meta-RL\n",
    "\n",
    "# simulates consecutive episodes of interaction between an agent and an environment to evaluate the performance of a trained model\n",
    "\n",
    "class RolloutStats(struct.PyTreeNode):\n",
    "    reward: jax.Array = jnp.asarray(0.0)  # accumulates the total rewards obtained during the rollout\n",
    "    length: jax.Array = jnp.asarray(0)    # counts the total number of steps taken across all episodes\n",
    "    episodes: jax.Array = jnp.asarray(0)  # tracks the number of completed episodes\n",
    "\n",
    "def rollout(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> RolloutStats:\n",
    "    # checks if the number of completed episodes is less than `num_consecutive_episodes`\n",
    "    def _cond_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "        return jnp.less(stats.episodes, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "\n",
    "        # APPLYING THE POLICY\n",
    "        dist, _, hstate = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                \"observation\": timestep.observation[None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "\n",
    "        # samples an action from the probability distribution output by the policy network; then remove any singleton dimensions from the action tensor\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        # advances the environment's state using the selected action; updates the `timestep`, which includes the new observation, reward, and done flag\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        # UPDATING STATISTICS AND REPACKING CARRY\n",
    "        stats = stats.replace(\n",
    "            reward=stats.reward + timestep.reward,\n",
    "            length=stats.length + 1,\n",
    "            episodes=stats.episodes + timestep.last(),\n",
    "        )\n",
    "        carry = (rng, stats, timestep, action, timestep.reward, hstate)\n",
    "\n",
    "        return carry\n",
    "\n",
    "    # ENVIRONMENT RESET AND INITIAL SETUP\n",
    "    timestep = env.reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    init_carry = (rng, RolloutStats(), timestep, prev_action, prev_reward, init_hstate)\n",
    "\n",
    "    # WHILE-LOOP EXEC\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "\n",
    "    return final_carry[1]  # specifically extracts the `RolloutStats` instance from the tuple, which now contains the total rewards accumulated, total length in terms of steps, and the number of episodes completed during the rollout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9hQW-TiuUkD"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR6Og6EAuV5d"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    env_id: str = \"MiniGrid-Empty-6x6\"\n",
    "    benchmark_id: Optional[str] = None\n",
    "    ruleset_id: Optional[int] = None\n",
    "    img_obs: bool = False\n",
    "    # agent\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 1024\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 256\n",
    "    # training\n",
    "    num_envs: int = 8192\n",
    "    num_steps: int = 16\n",
    "    update_epochs: int = 1\n",
    "    num_minibatches: int = 16\n",
    "    total_timesteps: int = 5_000_000\n",
    "    lr: float = 0.001\n",
    "    clip_eps: float = 0.2\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    eval_episodes: int = 40\n",
    "    seed: int = 42\n",
    "\n",
    "    def __post_init__(self):\n",
    "        num_devices = jax.local_device_count()\n",
    "        # splitting computation across all available devices\n",
    "        self.num_envs_per_device = self.num_envs // num_devices\n",
    "        self.total_timesteps_per_device = self.total_timesteps // num_devices\n",
    "        self.eval_episodes_per_device = self.eval_episodes // num_devices\n",
    "        assert self.num_envs % num_devices == 0\n",
    "        self.num_updates = self.total_timesteps_per_device // self.num_steps // self.num_envs_per_device\n",
    "        print(f\"Num devices: {num_devices}, Num updates: {self.num_updates}\")\n",
    "\n",
    "\n",
    "def make_states(config: TrainConfig):\n",
    "    # for learning rate scheduling\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config.num_minibatches * config.update_epochs)) / config.num_updates\n",
    "        return config.lr * frac\n",
    "\n",
    "    # setup environment\n",
    "    env, env_params = xminigrid.make(config.env_id)  # creates and environment based on env_id\n",
    "    env = GymAutoResetWrapper(env)  # for automatically resetting the environment when it reaches a terminal state\n",
    "\n",
    "    # for single-task XLand environments\n",
    "    if config.benchmark_id is not None:\n",
    "        assert \"XLand-MiniGrid\" in config.env_id, \"Benchmarks should be used only with XLand environments.\"\n",
    "        assert config.ruleset_id is not None, \"Ruleset ID should be specified for benchmarks usage.\"\n",
    "        benchmark = xminigrid.load_benchmark(config.benchmark_id)\n",
    "        env_params = env_params.replace(ruleset=benchmark.get_ruleset(config.ruleset_id))\n",
    "\n",
    "    # enabling image observations if needed\n",
    "    if config.img_obs:\n",
    "        from xminigrid.experimental.img_obs import RGBImgObservationWrapper\n",
    "\n",
    "        env = RGBImgObservationWrapper(env)\n",
    "\n",
    "    # setup training state\n",
    "    rng = jax.random.PRNGKey(config.seed)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    network = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "    )\n",
    "    # [batch_size, seq_len, ...]\n",
    "    init_obs = {\n",
    "        \"observation\": jnp.zeros((config.num_envs_per_device, 1, *env.observation_shape(env_params))),\n",
    "        \"prev_action\": jnp.zeros((config.num_envs_per_device, 1), dtype=jnp.int32),\n",
    "        \"prev_reward\": jnp.zeros((config.num_envs_per_device, 1)),\n",
    "    }\n",
    "    init_hstate = network.initialize_carry(batch_size=config.num_envs_per_device)\n",
    "\n",
    "    network_params = network.init(_rng, init_obs, init_hstate)\n",
    "\n",
    "    # optimizer setup\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config.max_grad_norm),\n",
    "        optax.inject_hyterparams(optax.adam)(learning_rate=linear_schedule, eps=1e-8),\n",
    "    )\n",
    "    train_state = TrainState.create(apply_fn=network.apply, params=network_params, tx=tx)\n",
    "\n",
    "    return rng, env, env_params, init_hstate, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BI7HJN4tUChC"
   },
   "outputs": [],
   "source": [
    "def make_train(\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    config: TrainConfig,\n",
    "):\n",
    "    @partial(jax.pmap, axis_name=\"devices\")\n",
    "    def train(\n",
    "        rng: jax.Array,\n",
    "        train_state: TrainState,\n",
    "        init_hstate: jax.Array,\n",
    "    ):\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config.num_envs_per_device)\n",
    "\n",
    "        timestep = jax.vmap(env.reset, in_axes=(None, 0))(env_params, reset_rng)\n",
    "        prev_action = jnp.zeros(config.num_envs_per_device, dtype=jnp.int32)\n",
    "        prev_reward = jnp.zeros(config.num_envs_per_device)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, _):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            # perform one step of interaction between the agent and the environment, updating the state based on the agent's action and the environment's response\n",
    "            def _env_step(runner_state, _):\n",
    "                rng, train_state, prev_timestep, prev_action, prev_reward, prev_hstate = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                dist, value, hstate = train_state.apply_fn(\n",
    "                    train_state.params,\n",
    "                    {\n",
    "                        # [batch_size, seq_len=1, ...]\n",
    "                        \"observation\": prev_timestep.observation[:, None],\n",
    "                        \"prev_action\": prev_action[:, None],\n",
    "                        \"prev_reward\": prev_reward[:, None],\n",
    "                    },\n",
    "                    prev_hstate,\n",
    "                )\n",
    "                action, log_prob = dist.sample_and_log_prob(seed=_rng)\n",
    "                # squeeze seq_len where possible\n",
    "                action, value, log_prob = action.squeeze(1), value.squeeze(1), log_prob.squeeze(1)\n",
    "\n",
    "                # STEP ENV\n",
    "                timestep = jax.vmap(env.step, in_axes=(None, 0, 0))(env_params, prev_timestep, action)\n",
    "\n",
    "                transition = Transition(\n",
    "                    done=timestep.last(),\n",
    "                    action=action,\n",
    "                    value=value,\n",
    "                    reward=timestep.reward,\n",
    "                    log_prob=log_prob,\n",
    "                    obs=prev_timestep.observation,\n",
    "                    prev_action=prev_action,\n",
    "                    prev_reward=prev_reward,\n",
    "                )\n",
    "                runner_state = (rng, train_state, timestep, action, timestep.reward, hstate)\n",
    "                return runner_state, transition\n",
    "\n",
    "            initial_hstate = runner_state[-1]\n",
    "            # transitions: [seq_len, batch_size, ...]\n",
    "            # applies `_env_step` across multiple steps\n",
    "            runner_state, transitions = jax.lax.scan(_env_step, runner_state, None, config.num_steps)\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            rng, train_state, timestep, prev_action, prev_reward, hstate = runner_state\n",
    "            # calculate value of the last step for boostrapping\n",
    "            _, last_val, _ = train_state.apply_fn(\n",
    "                train_state.params,\n",
    "                {\n",
    "                    \"observation\": timestep.observation[:, None],\n",
    "                    \"prev_action\": prev_action[:, None],\n",
    "                    \"prev_reward\": prev_reward[:, None],\n",
    "                },\n",
    "                hstate,\n",
    "            )\n",
    "            advantages, targets = calculate_gae(transitions, last_val.squeeze(1), config.gamma, config.gae_lambda)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            # handles the training for one epoch by processing several minibatches of data\n",
    "            def _update_epoch(update_state, _):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    init_hstate, transitions, advantages, targets = batch_info\n",
    "                    new_train_state, update_info = ppo_update_networks(\n",
    "                        train_state=train_state,\n",
    "                        transitions=transitions,\n",
    "                        init_hstate=init_hstate.squeeze(1),\n",
    "                        advantages=advantages,\n",
    "                        targets=targets,\n",
    "                        clip_eps=config.clip_eps,\n",
    "                        vf_coef=config.vf_coef,\n",
    "                        ent_coef=config.ent_coef,\n",
    "                    )\n",
    "                    return new_train_state, update_info\n",
    "\n",
    "                rng, train_state, init_hstate, transitions, advantages, targets = update_state\n",
    "\n",
    "                # MINIBATCHES PREPARATION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                permutation = jax.random.permutation(_rng, config.num_envs_per_device)\n",
    "                # [seq_len, batch_size, ...]\n",
    "                batch = (init_hstate, transitions, advantages, targets)\n",
    "                # [batch_size, seq_len, ...] as our model assumes\n",
    "                batch = jtu.tree_map(lambda x: x.swapaxes(0, 1), batch)\n",
    "\n",
    "                shuffled_batch = jtu.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "                # [num_minibatches, minibatch_size, ...]\n",
    "                minibatches = jtu.tree_map(\n",
    "                    lambda x: jnp.reshape(x, (config.num_minibatches, -1) + x.shape[1:]), shuffled_batch\n",
    "                )\n",
    "                train_state, update_info = jax.lax.scan(_update_minbatch, train_state, minibatches)\n",
    "\n",
    "                update_state = (rng, train_state, init_hstate, transitions, advantages, targets)\n",
    "                return update_state, update_info\n",
    "\n",
    "            # []"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
